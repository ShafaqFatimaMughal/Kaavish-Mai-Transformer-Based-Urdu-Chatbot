{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Libraries"
      ],
      "metadata": {
        "id": "pJvkLH_kJrpy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "150APM7qR3Pc",
        "outputId": "42bae698-e937-4cbc-c65d-6549334958a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting clean-text\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 566 kB/s \n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=77a58a949be4199bdcbf265f955941c71cc961fc56c2f0f4907ed9c0bc6af99d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting better-profanity\n",
            "  Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: better-profanity\n",
            "Successfully installed better-profanity-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install clean-text\n",
        "!pip install better-profanity\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "7DW-OFTdJ0II"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xInnuQ2fRfTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08366356-e451-4ff4-beb4-51837acfef3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from cleantext import clean\n",
        "from better_profanity import profanity\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cloning GitHub Repository"
      ],
      "metadata": {
        "id": "vOBocrD8Obbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShafaqFatimaMughal/Kaavish-Mai-Transformer-Based-Urdu-Chatbot.git\n",
        "path = \"/content/Kaavish-Mai-Transformer-Based-Urdu-Chatbot/Datasets/Initial\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPcUvZyAObFk",
        "outputId": "2537245f-1b64-4133-eda4-9572197342f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Kaavish-Mai-Transformer-Based-Urdu-Chatbot' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting and Cleaning Data"
      ],
      "metadata": {
        "id": "e3Wj_8aROfBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = path + \"/Reddit\"\n",
        "\n",
        "filenames = list()\n",
        "for root, dirs, files in os.walk(path):\n",
        "  for index, name in enumerate(files):\n",
        "        filename = os.path.join(root, name)\n",
        "        filenames.append(filename)"
      ],
      "metadata": {
        "id": "rRcWVt2mOzd0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opeining a sample file to extract columns\n",
        "columns = pd.read_csv(filenames[0], index_col=0).columns\n",
        "# Create new concatenated, clean DataFrame and setting its columns\n",
        "new_df = pd.DataFrame(columns = columns)\n",
        "\n",
        "# Iterating over all files in the repository\n",
        "for fname in filenames:\n",
        "  print(\"Processing\", fname+\"...\")\n",
        "  # Reading a dile in the directory\n",
        "  df = pd.read_csv(fname, index_col=0)\n",
        "\n",
        "  # Iterating over files dataframe\n",
        "  for index, row in tqdm(df.iterrows()):\n",
        "    # Creating new data to append to new dataframe\n",
        "    row_data = {}\n",
        "    # Iterating over all columns of a row and cleaning the data to add to the dict\n",
        "    for col in df.columns:\n",
        "      s = clean(\n",
        "                  row[col], no_emoji=True, lower=True,\n",
        "                  no_urls=True, no_emails=True, no_phone_numbers=True, no_punct = False,\n",
        "                  replace_with_url=\"\", replace_with_email=\"\", replace_with_phone_number=\"\"\n",
        "                )\n",
        "      row_data[col] = profanity.censor(s, \"\")\n",
        "    # Appending the data to the new dataframe\n",
        "    new_df = new_df.append(row_data, ignore_index=True)\n",
        "\n",
        "# Deleting rows with Null Values\n",
        "print(\"Before DropNA:\", new_df.shape)\n",
        "new_df.dropna(axis=0, how='any', inplace=True)\n",
        "print(\"Before DropNA:\", new_df.shape)\n",
        "\n",
        "# Naming and saving new dataset\n",
        "new_name = \"Reddit_clean.csv\"\n",
        "new_df.to_csv(new_name, index=False)\n",
        "files.download(new_name)\n",
        "\n",
        "new_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "GkiYf5ajAZek",
        "outputId": "3dfa6ba0-9f58-44f8-db20-b52873b8e1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/Kaavish-Mai-Transformer-Based-Urdu-Chatbot/Datasets/Initial/Reddit/Periods.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:12, 12.47s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2c51ef49fb10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                             )\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mrow_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofanity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Appending the data to the new dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/better_profanity/better_profanity.py\u001b[0m in \u001b[0;36mcensor\u001b[0;34m(self, text, censor_char)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCENSOR_WORDSET\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_censor_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hide_swear_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcensor_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_censor_words_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/better_profanity/better_profanity.py\u001b[0m in \u001b[0;36m_hide_swear_words\u001b[0;34m(self, text, censor_char)\u001b[0m\n\u001b[1;32m    183\u001b[0m             )\n\u001b[1;32m    184\u001b[0m             contains_swear_word, end_index = any_next_words_form_swear_word(\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mcur_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCENSOR_WORDSET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             )\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontains_swear_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/better_profanity/utils.py\u001b[0m in \u001b[0;36many_next_words_form_swear_word\u001b[0;34m(cur_word, words_indices, censor_words)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mword_with_separators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         )\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfull_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcensor_words\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfull_word_with_separators\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcensor_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/better_profanity/varying_string.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# We have no use case for this yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mlen_other\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen_other\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_min_len\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen_other\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amhFeNbiSNAg"
      },
      "outputs": [],
      "source": [
        "# # custom_badwords = ['happy', 'jolly', 'merry']\n",
        "# # profanity.load_censor_words(custom_badwords)\n",
        "\n",
        "# for fname in filenames:\n",
        "#   print(\"Processing\", fname+\"...\")\n",
        "#   df = pd.read_csv(fname, index_col=0)\n",
        "#   print(df)\n",
        "#   for ind in tqdm(df.index):\n",
        "#     for colname in df.columns[1:]:\n",
        "#       df[colname][ind] = df[colname][ind].replace(df[colname][ind], clean(\n",
        "#                               df[colname][ind], no_emoji=True, lower=False,\n",
        "#                               no_urls=True, no_emails=True, no_phone_numbers=True,\n",
        "#                               replace_with_url=\"\", replace_with_email=\"\", replace_with_phone_number=\"\" \n",
        "#                             ))\n",
        "      \n",
        "#       df[colname][ind] = profanity.censor(df[colname][ind], \"\")\n",
        "\n",
        "#   df.to_csv(fname, index=False)\n",
        "#   files.download(fname.split(\".\")[])\n",
        "# df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pJvkLH_kJrpy"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}